[
  {
    "question": "A chatbot asserts that 'Smith et al., 2022' found a 37% effect and even cites page numbers, but no such paper can be located in reputable databases. Which option best characterizes this failure mode?",
    "answers": [
      "Exposure to online misinformation - faithful reproduction of a dubious source",
      "Hallucination - confidently generated but nonexistent citation and statistics",
      "The black-box problem - reasoning is opaque rather than factually wrong",
      "Overfitting - memorization of training snippets leading to poor generalization"
    ],
    "correct": "Hallucination - confidently generated but nonexistent citation and statistics"
  },
  {
    "question": "Why did Amazon scrap its resume screening AI system (2014-2018)?",
    "answers": [
      "It was too expensive to maintain",
      "It couldn't guarantee bias elimination after downgrading women's resumes",
      "It struggled to scale effectively during hiring surges",
      "Competitors had developed far superior systems that made Amazon's technology obsolete"
    ],
    "correct": "It couldn't guarantee bias elimination after downgrading women's resumes"
  },
  {
    "question": "Which of the following is NOT a core principle of Ethical AI?",
    "answers": [
      "Transparency",
      "Explainability",
      "Human in the Loop",
      "Accountability"
    ],
    "correct": "Explainability"
  },
  {
    "question": "What are guardrails in AI systems?",
    "answers": [
      "Rules and safety mechanisms that prevent AI from generating harmful, biased, or inappropriate content",
      "Physical or operational restrictions placed around AI hardware deployments",
      "Comprehensive training programs and certification courses designed specifically for AI developers and architects",
      "Hardware limitations that cap maximum processing speed"
    ],
    "correct": "Rules and safety mechanisms that prevent AI from generating harmful, biased, or inappropriate content"
  },
  {
    "question": "What is the 'Black Box' problem in AI?",
    "answers": [
      "AI systems are painted black (honoring The Rolling Stones and their tribute to AI)",
      "We often can't see inside to understand how or why AI reached a conclusion",
      "AI systems are stored on protected servers with limited access",
      "The source code may be obscured by proprietary or highly technical design choices"
    ],
    "correct": "We often can't see inside to understand how or why AI reached a conclusion"
  },
  {
    "question": "Which item must be treated as PII in your dataset audit?",
    "answers": [
      "Persistent mobile advertising ID (e.g., IDFA/GAID) linked to a device",
      "Session telemetry tied to a random ID that resets each launch, with no user link",
      "Store-visit counts aggregated into 5 km tiles with at least 100 devices per tile",
      "Citywide average app session length for the last quarter"
    ],
    "correct": "Persistent mobile advertising ID (e.g., IDFA/GAID) linked to a device"
  },
  {
    "question": "Before using an AI assistant to troubleshoot proprietary code or handle PII, what is the best first step?",
    "answers": [
      "Anonymize the data and use any public AI tool.",
      "Trust the vendor's 'no training' / ZDR (Zero Data Retention) setting and continue.",
      "Check with HLV security and use an approved enterprise tool.",
      "Enable guardrails and avoid sharing full files."
    ],
    "correct": "Check with HLV security and use an approved enterprise tool."
  },
  {
    "question": "Which design change would most directly mitigate a Tay-like failure in a public-facing chatbot?",
    "answers": [
      "Disable learning from live user interactions and rely only on static pretraining (labeled data)",
      "Add robust input/output safety filters and adversarial testing to block abusive content",
      "Retrain on more diverse data while keeping the same public deployment setup",
      "Publish detailed model cards and usage guidelines to improve system transparency"
    ],
    "correct": "Add robust input/output safety filters and adversarial testing to block abusive content"
  },
  {
    "question": "You are drafting an email about employee salaries and plan to use a public AI writing assistant. What should you do?",
    "answers": [
      "Use fictional ranges or placeholders and avoid real names or figures; draft safely, then add specifics offline.",
      "Turn on the assistant's private/confidential mode and include exact salary figures to ensure precise, tailored wording.",
      "Share aggregated internal salary bands by role (e.g., $80k-$90k) since no single person is directly identified.",
      "Provide employee names and titles without amounts; the AI can infer appropriate ranges based on the profession, location, and experience."
    ],
    "correct": "Use fictional ranges or placeholders and avoid real names or figures; draft safely, then add specifics offline."
  },
  {
    "question": "On a public website not affiliated with your bank, government, or your company's approved tools, an AI chatbot asks for your national ID or passport number to 'verify your identity.' What should you do?",
    "answers": [
      "Provide it; online verification improves security and chatbots have guardrails.",
      "Decline and verify through an official channel; never share PII with public AI tools.",
      "Share only if the bot displays a privacy policy and promises encryption/incognito mode.",
      "Decline because identity checks should never be done online."
    ],
    "correct": "Decline and verify through an official channel; never share PII with public AI tools."
  },
  {
    "question": "An AI chatbot confidently states that your company's main competitor filed for bankruptcy yesterday. Your next step should be:",
    "answers": [
      "Immediately notify your leadership team",
      "Verify this information through official sources before taking any action",
      "Investigate whether your company should file for bankruptcy as well, based on the information provided by the AI chatbot",
      "Discuss it on social media for quick feedback"
    ],
    "correct": "Verify this information through official sources before taking any action"
  },
  {
    "question": "An AI tool drafts guidance for a high-stakes, jurisdiction-specific contract dispute and includes case and statute citations. What is the most appropriate next step?",
    "answers": [
      "Proceed if the citations look plausible, seem relevant to your facts, and pass a quick spot check.",
      "Verify every citation in primary sources and practice guides yourself, then act without consulting licensed counsel.",
      "Use it as a starting point: confirm each citation exists and applies, and get review from licensed counsel.",
      "Proceed when multiple sources are cited, the reasoning is coherent, and it appears tailored to your jurisdiction."
    ],
    "correct": "Use it as a starting point: confirm each citation exists and applies, and get review from licensed counsel."
  },
  {
    "question": "Per company policy requiring security approval for AI tools, your team wants to use an unapproved AI service to process internal documents that may include proprietary code and personal data. What is the most responsible first step?",
    "answers": [
      "Anonymize and share only small snippets using the tool's do-not-train setting, then proceed.",
      "Check with the security team for approved options or request a formal review before any use.",
      "Share only pseudocode and synthetic data with the tool while you await approval.",
      "Limit use to documents not marked highly confidential and avoid uploading PII."
    ],
    "correct": "Check with the security team for approved options or request a formal review before any use."
  },
  {
    "question": "You're preparing a query for a public AI assistant. Which item is the clearest example of proprietary code you should not share?",
    "answers": [
      "A 40-line function from your company's private repo implementing discount eligibility rules",
      "Pseudocode describing your firm's unique dynamic-pricing strategy (no actual code shown)",
      "A high-level component architecture diagram with no source, sample data, secrets, or identifiers",
      "A short snippet copied from an open-source library's official documentation"
    ],
    "correct": "A 40-line function from your company's private repo implementing discount eligibility rules"
  },
  {
    "question": "What are red flags for recognizing AI hallucinations?",
    "answers": [
      "Responses that are overly concise or lack detail",
      "Overly specific details without sources, precise quotes without attribution",
      "Responses that appear faster than expected with no delay",
      "Excessively long paragraphs without clear structure"
    ],
    "correct": "Overly specific details without sources, precise quotes without attribution"
  },
  {
    "question": "How do AI guardrails typically work to prevent harmful outputs?",
    "answers": [
      "They filter and block outputs during the inference phase before users see them",
      "They prevent biased data from entering the training dataset initially",
      "They automatically retrain the model when harmful patterns are detected",
      "They monitor user behavior and restrict access to AI tools based on usage patterns"
    ],
    "correct": "They filter and block outputs during the inference phase before users see them"
  },
  {
    "question": "You are preparing customer-support data for an AI assistant. Which practice best demonstrates data minimization?",
    "answers": [
      "Share only the fields required for the task, removing unrelated identifiers and sensitive attributes",
      "Share the full dataset but replace names and emails with hashes to protect identity",
      "Share all records unchanged but enforce role-based access controls and detailed audit logs",
      "Provide aggregated summaries for analysis while retaining raw, identifiable data for verification purposes"
    ],
    "correct": "Share only the fields required for the task, removing unrelated identifiers and sensitive attributes"
  },
  {
    "question": "What was identified as the primary root cause of bias in Amazon's resume-screening system?",
    "answers": [
      "Male-dominated training data from a decade of resumes taught the model to prefer men.",
      "The system's opaque design limited HR's ability to meaningfully audit or adjust its decisions.",
      "Overreliance on keyword counts rather than a holistic review of qualifications and experience.",
      "Initial heuristic rules inadvertently penalized terms associated with women's colleges and activities."
    ],
    "correct": "Male-dominated training data from a decade of resumes taught the model to prefer men."
  },
  {
    "question": "A colleague plans to use a third-party AI service to transcribe next week's client meeting. What is the best guidance?",
    "answers": [
      "Use the service's private/confidential mode under the company NDA, record the meeting, and share the transcript internally; notify participants afterward so they can flag any concerns.",
      "Before recording, verify the service is security-approved (or use an internal approved tool), explain how the audio will be used and stored, and obtain explicit consent from all participants; otherwise, do not record.",
      "Record only agenda items and action points, keep the files internal, and distribute a concise summary; treat this as adequate risk reduction without requiring prior explicit consent.",
      "Proceed if the vendor provides end-to-end encryption, automatic deletion, and compliant terms; provide an opt-out form after the meeting for anyone uncomfortable with the recording."
    ],
    "correct": "Before recording, verify the service is security-approved (or use an internal approved tool), explain how the audio will be used and stored, and obtain explicit consent from all participants; otherwise, do not record."
  },
  {
    "question": "In your company's AI chat assistant, the bot asks you inside the chat to paste your password to 'verify identity,' rather than redirecting you to the official company SSO login page. What should you do?",
    "answers": [
      "Provide a masked or partial password so the bot can proceed",
      "Do not share credentials; authenticate only via the official SSO/login page and report the request to security",
      "Share the password because the tool is security-team approved, encrypted, and says it won't store credentials",
      "Send a temporary or changed password now and reset it afterward to minimize risk"
    ],
    "correct": "Do not share credentials; authenticate only via the official SSO/login page and report the request to security"
  },
  {
    "question": "You ask the same AI the same medical question multiple times and receive confident but contradictory answers with no citations. What is the safest, most responsible action?",
    "answers": [
      "Ask the AI for citations; if it provides them, trust the answer",
      "Assume hallucination; confirm via authoritative sources or a clinician before acting",
      "Keep rephrasing until responses converge, then accept that result",
      "Consult peer-reviewed guidelines and proceed if one answer matches them"
    ],
    "correct": "Assume hallucination; confirm via authoritative sources or a clinician before acting"
  }
]
