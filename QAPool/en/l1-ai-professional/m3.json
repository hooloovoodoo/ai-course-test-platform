[
  {
    "question": "Which scenario best illustrates 'jagged intelligence' as described in Module 3?",
    "answers": [
      "It clusters 1,000 support tickets into themes in minutes but sometimes gets simple counts wrong.",
      "It delivers consistent quality across tasks whenever the prompt is very detailed; differences mostly reflect how well the request is written.",
      "It responds more slowly without specialized hardware, yet its accuracy stays consistent across task types regardless of what it is asked to do.",
      "It improves uniformly across skills as it sees more data, so performance gaps between tasks fade and results become predictably even."
    ],
    "correct": "It clusters 1,000 support tickets into themes in minutes but sometimes gets simple counts wrong."
  },
  {
    "question": "Based on Module 3’s jagged intelligence concept—and assuming a general-purpose text model without external tools or live data—which task is AI most reliable at?",
    "answers": [
      "Counting letters in a word without mistakes",
      "Identifying themes across large sets of customer reviews",
      "Choosing the perfect tone for a specific executive",
      "Accurately interpreting a complex floor plan after a 90° rotation"
    ],
    "correct": "Identifying themes across large sets of customer reviews"
  },
  {
    "question": "Which task is AI likely to struggle with?",
    "answers": [
      "Writing first drafts of emails",
      "Generating multiple variations of marketing copy",
      "Basic counting and simple math",
      "Summarizing lengthy documents"
    ],
    "correct": "Basic counting and simple math"
  },
  {
    "question": "What is the 'Golden Rule' when working with AI?",
    "answers": [
      "Always use the most expensive AI tool available",
      "You are the professional, AI is the assistant - review everything AI produces",
      "Trust AI completely for all business decisions",
      "Never use AI for any work-related tasks"
    ],
    "correct": "You are the professional, AI is the assistant - review everything AI produces"
  },
  {
    "question": "An AI assistant says, “According to a 2024 HBR study by Dr. J. Lawson, 87% of firms…” but you can’t find the study, author, or source after checking. This is best classified as:",
    "answers": [
      "Hallucination: confident, plausible but fabricated claim or citation",
      "Outdated information: previously accurate content presented as current",
      "Bias: output reflecting stereotypes or unfair assumptions",
      "Jagged intelligence: uneven strengths and weaknesses across tasks"
    ],
    "correct": "Hallucination: confident, plausible but fabricated claim or citation"
  },
  {
    "question": "Why are AI hallucinations particularly dangerous?",
    "answers": [
      "They cause the AI system to crash",
      "They are easy to detect immediately",
      "AI sounds completely convincing even when it's wrong",
      "They only occur in consumer AI tools"
    ],
    "correct": "AI sounds completely convincing even when it's wrong"
  },
  {
    "question": "Which of the following is a common cause of AI hallucinations?",
    "answers": [
      "Using enterprise-grade AI tools",
      "Training data gaps where AI lacks information on certain topics",
      "Asking questions that are too simple",
      "Having a stable internet connection"
    ],
    "correct": "Training data gaps where AI lacks information on certain topics"
  },
  {
    "question": "You’re drafting a customer policy notice where accuracy is critical and reducing hallucination risk matters. Which temperature choice and expected effect are most appropriate?",
    "answers": [
      "Use a low temperature; outputs are more deterministic and less variable, reducing but not eliminating hallucination risk.",
      "Use a high temperature; greater creativity makes answers fuller and therefore more reliable.",
      "Use a medium temperature; it balances creativity and accuracy and is preferred for critical factual work.",
      "Use a low temperature; the model will automatically retrieve and verify external sources."
    ],
    "correct": "Use a low temperature; outputs are more deterministic and less variable, reducing but not eliminating hallucination risk."
  },
  {
    "question": "Which behavior best matches the module’s hallucination red flag described as 'Citations without links or details'?",
    "answers": [
      "The model cites “a 2024 MIT study by Dr. Lee Chang” and lists only “Source: MIT” without a link, journal, or publication details.",
      "The model reports 74% and a three-week timeframe and includes a working link to an industry survey with the publisher and methodology appendix.",
      "The model states assumptions, uses cautious terms like “may” and “estimates,” and recommends independent verification of figures.",
      "The model notes its knowledge cutoff is 2023 and advises checking for updates before relying on claims about very recent events."
    ],
    "correct": "The model cites “a 2024 MIT study by Dr. Lee Chang” and lists only “Source: MIT” without a link, journal, or publication details."
  },
  {
    "question": "An AI draft says, 'According to a 2024 study...' but gives no author, journal, or link. What is the best next step?",
    "answers": [
      "Ask the AI for a full citation and include it if it looks complete",
      "Treat it as a red flag: verify externally; if you can’t confirm, remove or rephrase the claim",
      "Keep the claim but hedge with 'one study suggests' until more info is available",
      "Trust it if the tool is enterprise-grade and the response sounds professional"
    ],
    "correct": "Treat it as a red flag: verify externally; if you can’t confirm, remove or rephrase the claim"
  },
  {
    "question": "Which sentence is the clearest red-flag indicator of gender bias in AI-generated content?",
    "answers": [
      "Each manager should submit their report by Friday.",
      "The CEO should submit his quarterly report by Friday.",
      "Each manager should submit his or her report by Friday.",
      "All managers should submit their reports by Friday."
    ],
    "correct": "The CEO should submit his quarterly report by Friday."
  },
  {
    "question": "What is PII that should NEVER be included in an AI prompt?",
    "answers": [
      "General industry statistics",
      "Full names combined with addresses, phone numbers, or Social Security numbers",
      "Publicly available company information",
      "Generic job titles"
    ],
    "correct": "Full names combined with addresses, phone numbers, or Social Security numbers"
  },
  {
    "question": "What does PHI stand for in the context of data privacy?",
    "answers": [
      "Personal Human Interaction",
      "Protected Health Information",
      "Private Historical Information",
      "Public Health Index"
    ],
    "correct": "Protected Health Information"
  },
  {
    "question": "Assume you're using an enterprise-grade AI tool; organizational privacy rules still apply. Which of the following should NOT be included in a work prompt?",
    "answers": [
      "Text from the product FAQ available to customers",
      "An anonymized table of quarterly sales by region (no customer identifiers)",
      "A spreadsheet of customer email addresses (no names included)",
      "A generic performance-review template with placeholder fields"
    ],
    "correct": "A spreadsheet of customer email addresses (no names included)"
  },
  {
    "question": "You're using an AI tool to draft a performance review. To protect privacy while still getting a useful draft, what is the best approach?",
    "answers": [
      "Use initials and the department instead of the full name.",
      "Ask for a generic draft with placeholders and add real details later.",
      "Include the real name but label the prompt as confidential.",
      "Use a pseudonym and keep a secure mapping file to the real person."
    ],
    "correct": "Ask for a generic draft with placeholders and add real details later."
  },
  {
    "question": "The AI Output Evaluation Framework uses five checkpoints. Which option lists them exactly as named in the module?",
    "answers": [
      "Accuracy, Hallucinations, Tone, PII/Confidentiality, Relevance/Context",
      "Accuracy, Hallucinations, Bias, PII/Confidentiality, Relevance/Jaggedness",
      "Accuracy, Sources, Bias, Compliance, Relevance/Context",
      "Accuracy, Citations, Bias, Privacy/PII, Relevance/Context Fit"
    ],
    "correct": "Accuracy, Hallucinations, Bias, PII/Confidentiality, Relevance/Jaggedness"
  },
  {
    "question": "According to this module's accountability guidance, as an individual professional, who is responsible for any AI-generated content you share, publish, or act on?",
    "answers": [
      "You, the professional who uses and shares it",
      "Your organization's legal or compliance team",
      "The IT department that deployed the tool",
      "The AI vendor that built the model"
    ],
    "correct": "You, the professional who uses and shares it"
  },
  {
    "question": "Your AI assistant (no browsing or real-time tools) confidently summarizes a product launch reported 'last week' with precise figures. What’s the best next step?",
    "answers": [
      "Verify against current primary sources and confirm dates; the model lacks live data.",
      "Trust the summary if the citations look real and are well-formatted.",
      "Ask the AI to re-check the dates and add links, then proceed with its answer.",
      "Lower the temperature to reduce creativity and keep the numbers as given."
    ],
    "correct": "Verify against current primary sources and confirm dates; the model lacks live data."
  },
  {
    "question": "Which task is appropriate to use AI for?",
    "answers": [
      "Final calculations for financial reports without verification",
      "Legal advice for important contracts",
      "First drafts and outlines",
      "Decisions requiring company-specific political context"
    ],
    "correct": "First drafts and outlines"
  },
  {
    "question": "What lesson does the Air Canada chatbot case teach about AI use?",
    "answers": [
      "AI chatbots are always accurate for customer service",
      "Organizations remain accountable for AI outputs even when used as support tools",
      "Companies should never use AI for customer communication",
      "AI chatbots cannot provide legally binding information"
    ],
    "correct": "Organizations remain accountable for AI outputs even when used as support tools"
  },
  {
    "question": "In the 'Legal Cases Fabrication' example, why were the lawyers sanctioned?",
    "answers": [
      "They cited court opinions that AI had invented and do not exist.",
      "They cited real opinions from the wrong jurisdiction.",
      "They failed to disclose that AI helped draft the brief.",
      "They relied on outdated precedents due to the model's cutoff."
    ],
    "correct": "They cited court opinions that AI had invented and do not exist."
  },
  {
    "question": "Your AI draft for a developer job post reads: 'We're looking for a rockstar who can crush it in a fast-paced environment. Must be a culture fit. Minimal onboarding provided.' Which prompt revision best aligns with the module's guidance to reduce bias and clarify expectations?",
    "answers": [
      "Use inclusive, gender-neutral language; remove hype terms; avoid 'culture fit' in favor of clear role criteria; define what 'fast-paced' means; include onboarding and flexibility details.",
      "Use inclusive language and drop hype terms; keep 'culture fit' to reflect values; mention fast pace briefly; treat onboarding as optional to keep the post concise.",
      "Keep a neutral tone and avoid slang; stress high performance and ownership; leave 'fast-paced' broad; omit onboarding and flexibility to maintain a focused, results-first message.",
      "Use gender-neutral language and define fast pace with one example; retain 'culture fit' to screen applicants; add a brief note on accessibility and hybrid work options."
    ],
    "correct": "Use inclusive, gender-neutral language; remove hype terms; avoid 'culture fit' in favor of clear role criteria; define what 'fast-paced' means; include onboarding and flexibility details."
  },
  {
    "question": "What does AI typically struggle with regarding spatial reasoning?",
    "answers": [
      "Generating text descriptions of spaces",
      "Mental rotation tasks and understanding relative positions of objects",
      "Creating bullet point lists",
      "Formatting information into tables"
    ],
    "correct": "Mental rotation tasks and understanding relative positions of objects"
  },
  {
    "question": "An AI draft states: 'According to a 2024 Harvard Business Review study by Dr. Jennifer Lawson, 87% of firms using AI chatbots cut support costs by 42% (see hbr.org/2024/07/roi-conversational-ai).' Before adding this to your internal report, what is the best next step per the verification checklist?",
    "answers": [
      "Treat the details as likely accurate because they include precise numbers and a professional-sounding citation, and include them with a brief cautionary note.",
      "Independently verify the study and statistics by searching exact titles, authors, and quotes; cross-check with reliable sources and remove anything unverifiable.",
      "Use the numbers for internal drafts without delay, and plan to verify them only if the content will be shared externally or published.",
      "Open the provided link to confirm it loads, and if the page appears legitimate, proceed to use the citation and statistics as written."
    ],
    "correct": "Independently verify the study and statistics by searching exact titles, authors, and quotes; cross-check with reliable sources and remove anything unverifiable."
  }
]